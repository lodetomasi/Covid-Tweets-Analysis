{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started creating TweetsText-03-11-19\n",
      "Started creating TweetsText-03-18-13\n",
      "Started creating TweetsText-03-14-04\n",
      "Started creating TweetsText-03-15-07\n",
      "Started creating TweetsText-03-19-10\n",
      "Started creating TweetsText-03-18-19\n",
      "Started creating TweetsText-03-16-01\n",
      "Started creating TweetsText-03-11-13\n",
      "Started creating TweetsText-03-10-10\n",
      "Started creating TweetsText-03-13-22\n",
      "Started creating TweetsText-03-14-13\n",
      "Started creating TweetsText-03-18-04\n",
      "Started creating TweetsText-03-13-01\n",
      "Started creating TweetsText-03-19-07\n",
      "Started creating TweetsText-03-16-22\n",
      "Started creating TweetsText-03-15-10\n",
      "Started creating TweetsText-03-11-04\n",
      "Started creating TweetsText-03-14-19\n",
      "Started creating TweetsText-03-10-07\n",
      "Started creating TweetsText-03-13-07\n",
      "Started creating TweetsText-03-19-01\n",
      "Started creating TweetsText-03-17-19\n",
      "Started creating TweetsText-03-12-04\n",
      "Started creating TweetsText-03-16-10\n",
      "Started creating TweetsText-03-15-22\n",
      "Started creating TweetsText-03-10-01\n",
      "Started creating TweetsText-03-17-13\n",
      "Started creating TweetsText-03-10-22\n",
      "Started creating TweetsText-03-13-10\n",
      "Started creating TweetsText-03-12-13\n",
      "Started creating TweetsText-03-15-01\n",
      "Started creating TweetsText-03-16-07\n",
      "Started creating TweetsText-03-19-22\n",
      "Started creating TweetsText-03-17-04\n",
      "Started creating TweetsText-03-12-19\n",
      "Started creating TweetsText-03-11-07\n",
      "Started creating TweetsText-03-10-04\n",
      "Started creating TweetsText-03-15-19\n",
      "Started creating TweetsText-03-14-10\n",
      "Started creating TweetsText-03-17-22\n",
      "Started creating TweetsText-03-18-07\n",
      "Started creating TweetsText-03-19-04\n",
      "Started creating TweetsText-03-15-13\n",
      "Started creating TweetsText-03-12-01\n",
      "Started creating TweetsText-03-12-22\n",
      "Started creating TweetsText-03-11-10\n",
      "Started creating TweetsText-03-17-01\n",
      "Started creating TweetsText-03-19-19\n",
      "Started creating TweetsText-03-10-13\n",
      "Started creating TweetsText-03-18-10\n",
      "Started creating TweetsText-03-14-07\n",
      "Started creating TweetsText-03-10-19\n",
      "Started creating TweetsText-03-15-04\n",
      "Started creating TweetsText-03-19-13\n",
      "Started creating TweetsText-03-16-04\n",
      "Started creating TweetsText-03-13-19\n",
      "Started creating TweetsText-03-18-22\n",
      "Started creating TweetsText-03-17-07\n",
      "Started creating TweetsText-03-13-13\n",
      "Started creating TweetsText-03-14-01\n",
      "Started creating TweetsText-03-12-10\n",
      "Started creating TweetsText-03-11-22\n",
      "Started creating TweetsText-03-11-01\n",
      "Started creating TweetsText-03-16-13\n",
      "Started creating TweetsText-03-14-22\n",
      "Started creating TweetsText-03-17-10\n",
      "Started creating TweetsText-03-16-19\n",
      "Started creating TweetsText-03-18-01\n",
      "Started creating TweetsText-03-13-04\n",
      "Started creating TweetsText-03-12-07\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import heapq\n",
    "import os\n",
    "import pandas as pd \n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import copy\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")   #this HUGELY improves exec time\n",
    "\n",
    "#%%\n",
    "\n",
    "def decontracted(phrase): #It helps the stemmer to recognize stuff\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def text_preprocessing(input_text):\n",
    "    input_text = re.sub(\"(RT )?@[A-Za-z0-9_]+\",\"\", input_text) #strips RTs and tags\n",
    "    input_text = re.sub(r'http\\S+', '', input_text)    #strips URLs\n",
    "    input_text = input_text.lower()    #to LowerCase\n",
    "    input_text = decontracted(input_text)     #expands contractions\n",
    "    input_text = re.sub(\"[^a-zA-Z ]+\", \" \", input_text).strip()    #strips numbers, punctuation and leading/ending spaces\n",
    "    tokenized = nltk.word_tokenize (input_text)    #tokenizes    \n",
    "    tokenized = [i for i in tokenized if not i in cachedStopWords]  #remove stopwords\n",
    "    stemmer= PorterStemmer()\n",
    "    tokenized = [stemmer.stem(word) for word in tokenized]   #Porter stemming\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "def chiavi():\n",
    "    chiavi = []\n",
    "    rad = 'TweetsText-03-'\n",
    "    for i in range(1,31):\n",
    "        for j in range (1,25,3):\n",
    "            chiavi.append(rad + str(i).zfill(2) + '-' + str(j).zfill(2) + '.txt')\n",
    "    return chiavi\n",
    "\n",
    "def time_window(n):\n",
    "    if (n==0):\n",
    "        return chiavi()[:80]\n",
    "    else:\n",
    "        return chiavi()[40*n:40*n+80]\n",
    "    \n",
    "def fill_series(diz,window):    #Takes the most common 100k terms and fills their timeseries with 0s in hours in which they are not used\n",
    "    timeseries = nested_dict()\n",
    "    window = time_window(window)\n",
    "    for term in diz:\n",
    "        #print('The normalized frequency of the term \\'{}\\' over time is: {}'.format(term,dict.__repr__(tf[term]).replace('TweetsText-','')))\n",
    "        timeseries[term] = copy.deepcopy(tf[term])\n",
    "        for key, value in list(timeseries[term].items()):\n",
    "            if key not in window:\n",
    "                del timeseries[term][key]\n",
    "        for el in window:\n",
    "            if el not in timeseries[term]:\n",
    "                timeseries[term][el] = 0\n",
    "    return timeseries\n",
    "    \n",
    "\n",
    "#%% Opens the json files contained in the jsonl.gz archives, processes them and create some partial output txt files \n",
    "\n",
    "path = r'/Users/lorenzodetomasi/Desktop/esame_stilo/prova'\n",
    "#df = pd.DataFrame()\n",
    "#start = time.time()\n",
    "\n",
    "for filename in glob.glob(os.path.join(path, '*.jsonl.gz')):\n",
    "    \n",
    "    nome = 'TweetsText' + filename[-18:-9]    #It will be  '-MM-DD-HH'  (Month-Day-Hour)    \n",
    "    print(\"Started creating {}\".format(nome))\n",
    "    \n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "     if (nome + '.txt') not in os.listdir(path):   #Checks if this one hasn't already been processed\n",
    "      with open(path + '//' + nome + '.txt', 'w') as g:\n",
    "        for jsonObj in f:    #Each tweet in a file is a different json object\n",
    "            tweetDict = json.loads(jsonObj)\n",
    "            if \"retweeted_status\" in tweetDict:   #retweets and normal tweets have a DIFFERENT STRUCTURE! the former's \"full_text\" is truncated, so it's necessary to load the original tweet\n",
    "                g.write('\\n'.join(text_preprocessing(tweetDict['retweeted_status']['full_text'])))\n",
    "            else:\n",
    "                g.write('\\n'.join(text_preprocessing(tweetDict['full_text'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing TweetsText-03-11-04.txt\n",
      "Started processing TweetsText-03-16-19.txt\n",
      "Started processing TweetsText-03-11-10.txt\n",
      "Started processing TweetsText-03-13-01.txt\n",
      "Started processing TweetsText03-11-22.txt\n",
      "Started processing TweetsText03-18-13.txt\n",
      "Started processing TweetsText03-18-07.txt\n",
      "Started processing TweetsText03-14-07.txt\n",
      "Started processing TweetsText03-14-13.txt\n",
      "Started processing TweetsText-03-11-13.txt\n",
      "Started processing TweetsText-03-11-07.txt\n",
      "Started processing TweetsText03-18-04.txt\n",
      "Started processing TweetsText03-18-10.txt\n",
      "Started processing TweetsText03-16-01.txt\n",
      "Started processing TweetsText-03-14-22.txt\n",
      "Started processing TweetsText-03-18-22.txt\n",
      "Started processing TweetsText03-13-19.txt\n",
      "Started processing TweetsText03-14-10.txt\n",
      "Started processing TweetsText03-14-04.txt\n",
      "Started processing TweetsText-03-13-13.txt\n",
      "Started processing TweetsText-03-13-07.txt\n",
      "Started processing TweetsText03-11-19.txt\n",
      "Started processing TweetsText03-16-10.txt\n",
      "Started processing TweetsText03-16-04.txt\n",
      "Started processing TweetsText03-18-01.txt\n",
      "Started processing TweetsText-03-16-22.txt\n",
      "Started processing TweetsText03-14-01.txt\n",
      "Started processing TweetsText03-13-22.txt\n",
      "Started processing TweetsText-03-18-19.txt\n",
      "Started processing TweetsText-03-11-01.txt\n",
      "Started processing TweetsText-03-13-04.txt\n",
      "Started processing TweetsText-03-14-19.txt\n",
      "Started processing TweetsText-03-13-10.txt\n",
      "Started processing TweetsText03-16-07.txt\n",
      "Started processing TweetsText03-16-13.txt\n",
      "Started processing TweetsText-03-12-22.txt\n",
      "Started processing TweetsText03-10-01.txt\n",
      "Started processing TweetsText03-19-19.txt\n",
      "Started processing TweetsText03-15-19.txt\n",
      "Started processing TweetsText03-12-10.txt\n",
      "Started processing TweetsText03-12-04.txt\n",
      "Started processing TweetsText-03-17-13.txt\n",
      "Started processing TweetsText-03-17-07.txt\n",
      "Started processing TweetsText03-12-07.txt\n",
      "Started processing TweetsText03-12-13.txt\n",
      "Started processing TweetsText-03-19-01.txt\n",
      "Started processing TweetsText-03-17-04.txt\n",
      "Started processing TweetsText-03-10-19.txt\n",
      "Started processing TweetsText-03-17-10.txt\n",
      "Started processing TweetsText03-17-22.txt\n",
      "Started processing TweetsText-03-15-01.txt\n",
      "Started processing TweetsText03-10-07.txt\n",
      "Started processing TweetsText03-10-13.txt\n",
      "Started processing TweetsText-03-17-01.txt\n",
      "Started processing TweetsText-03-19-10.txt\n",
      "Started processing TweetsText-03-19-04.txt\n",
      "Started processing TweetsText03-15-22.txt\n",
      "Started processing TweetsText03-19-22.txt\n",
      "Started processing TweetsText-03-15-04.txt\n",
      "Started processing TweetsText-03-12-19.txt\n",
      "Started processing TweetsText-03-15-10.txt\n",
      "Started processing TweetsText03-17-19.txt\n",
      "Started processing TweetsText03-10-10.txt\n",
      "Started processing TweetsText03-10-04.txt\n",
      "Started processing TweetsText03-12-01.txt\n",
      "Started processing TweetsText-03-10-22.txt\n",
      "Started processing TweetsText-03-19-07.txt\n",
      "Started processing TweetsText-03-19-13.txt\n",
      "Started processing TweetsText-03-15-13.txt\n",
      "Started processing TweetsText-03-15-07.txt\n",
      "Started processing TweetsText-03-15-22.txt\n",
      "Started processing TweetsText03-19-10.txt\n",
      "Started processing TweetsText03-19-04.txt\n",
      "Started processing TweetsText03-17-01.txt\n",
      "Started processing TweetsText03-15-04.txt\n",
      "Started processing TweetsText03-12-19.txt\n",
      "Started processing TweetsText03-15-10.txt\n",
      "Started processing TweetsText-03-19-22.txt\n",
      "Started processing TweetsText-03-10-07.txt\n",
      "Started processing TweetsText-03-10-13.txt\n",
      "Started processing TweetsText03-19-07.txt\n",
      "Started processing TweetsText03-19-13.txt\n",
      "Started processing TweetsText03-15-13.txt\n",
      "Started processing TweetsText03-15-07.txt\n",
      "Started processing TweetsText-03-17-19.txt\n",
      "Started processing TweetsText-03-10-10.txt\n",
      "Started processing TweetsText-03-10-04.txt\n",
      "Started processing TweetsText03-10-22.txt\n",
      "Started processing TweetsText-03-12-01.txt\n",
      "Started processing TweetsText03-17-13.txt\n",
      "Started processing TweetsText03-17-07.txt\n",
      "Started processing TweetsText-03-19-19.txt\n",
      "Started processing TweetsText-03-10-01.txt\n",
      "Started processing TweetsText03-12-22.txt\n",
      "Started processing TweetsText-03-15-19.txt\n",
      "Started processing TweetsText-03-12-10.txt\n",
      "Started processing TweetsText-03-12-04.txt\n",
      "Started processing TweetsText03-17-04.txt\n",
      "Started processing TweetsText03-10-19.txt\n",
      "Started processing TweetsText03-17-10.txt\n",
      "Started processing TweetsText03-19-01.txt\n",
      "Started processing TweetsText03-15-01.txt\n",
      "Started processing TweetsText-03-17-22.txt\n",
      "Started processing TweetsText-03-12-07.txt\n",
      "Started processing TweetsText-03-12-13.txt\n",
      "Started processing TweetsText-03-18-01.txt\n",
      "Started processing TweetsText-03-11-19.txt\n",
      "Started processing TweetsText-03-16-10.txt\n",
      "Started processing TweetsText-03-16-04.txt\n",
      "Started processing TweetsText-03-14-01.txt\n",
      "Started processing TweetsText03-16-22.txt\n",
      "Started processing TweetsText03-13-13.txt\n",
      "Started processing TweetsText03-13-07.txt\n",
      "Started processing TweetsText-03-16-07.txt\n",
      "Started processing TweetsText-03-16-13.txt\n",
      "Started processing TweetsText03-11-01.txt\n",
      "Started processing TweetsText03-18-19.txt\n",
      "Started processing TweetsText-03-13-22.txt\n",
      "Started processing TweetsText03-13-04.txt\n",
      "Started processing TweetsText03-14-19.txt\n",
      "Started processing TweetsText03-13-10.txt\n",
      "Started processing TweetsText-03-18-13.txt\n",
      "Started processing TweetsText-03-18-07.txt\n",
      "Started processing TweetsText-03-14-07.txt\n",
      "Started processing TweetsText-03-14-13.txt\n",
      "Started processing TweetsText03-11-04.txt\n",
      "Started processing TweetsText03-16-19.txt\n",
      "Started processing TweetsText03-11-10.txt\n",
      "Started processing TweetsText-03-11-22.txt\n",
      "Started processing TweetsText03-13-01.txt\n",
      "Started processing TweetsText03-14-22.txt\n",
      "Started processing TweetsText-03-16-01.txt\n",
      "Started processing TweetsText-03-18-04.txt\n",
      "Started processing TweetsText-03-18-10.txt\n",
      "Started processing TweetsText-03-13-19.txt\n",
      "Started processing TweetsText-03-14-10.txt\n",
      "Started processing TweetsText-03-14-04.txt\n",
      "Started processing TweetsText03-18-22.txt\n",
      "Started processing TweetsText03-11-13.txt\n",
      "Started processing TweetsText03-11-07.txt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PiecewiseAggregateApproximation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c61356d1430f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mPAA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPiecewiseAggregateApproximation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdiz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtimeseries1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdiz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PiecewiseAggregateApproximation' is not defined"
     ]
    }
   ],
   "source": [
    "import collections \n",
    "path = r'/Users/lorenzodetomasi/Desktop/esame_stilo/prova'\n",
    "def nested_dict():\n",
    "    return collections.defaultdict(nested_dict)\n",
    "\n",
    "max_freq={}\n",
    "idf={}\n",
    "tf=nested_dict()\n",
    "docs = 0\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "  if filename.endswith(\".txt\"): \n",
    "    print(\"Started processing {}\".format(filename))\n",
    "    docs+=1\n",
    "    max_freq[filename] = 0\n",
    "    \n",
    "    freq = {}\n",
    "    with open(path + '/' + filename) as f:\n",
    "        words = f.read().splitlines()\n",
    "\n",
    "    for word in words:                                                    \n",
    "        if word not in freq:\n",
    "            freq[word] = 0\n",
    "            if word in idf:\n",
    "                idf[word]+=1\n",
    "            else:\n",
    "                idf[word] = 1\n",
    "        freq[word] += 1\n",
    "        max_freq[filename] = max(max_freq[filename],freq[word])\n",
    "                    \n",
    "    for word in words:\n",
    "       tf[word][filename]=freq[word]/max_freq[filename]\n",
    "   \n",
    "for a in idf:\n",
    "    idf[a]=math.log(docs/idf[a],2)   \n",
    "    \n",
    "tf_idf=copy.deepcopy(tf)\n",
    "sums={}\n",
    "for a in tf_idf:\n",
    "    for b in tf_idf[a]:\n",
    "            tf_idf[a][b] = tf_idf[a][b]*idf[a]    \n",
    "    sums[a] = sum(tf_idf[a].values()) \n",
    "    \n",
    "#top100k = heapq.nlargest(100000, sums, key=sums.__getitem__)  #Only top 100k tokens are necessary \n",
    "n = 100000     \n",
    "top100k = dict(collections.Counter(sums).most_common(n))                                                            \n",
    "#print('\\n Top {} words: '.format(n))\n",
    "#print(dict(collections.Counter(sums).most_common(n)))\n",
    "timeseries1 = fill_series(top100k,0)\n",
    "timeseries2 = fill_series(top100k,1)\n",
    "timeseries3 = fill_series(top100k,2)\n",
    "timeseries4 = fill_series(top100k,3)\n",
    "timeseries5 = fill_series(top100k,4)\n",
    "\n",
    "PAA = pd.DataFrame()\n",
    "transformer = PiecewiseAggregateApproximation(window_size=8)\n",
    "for diz in timeseries1:\n",
    "    for key, value in diz.items():\n",
    "        PAA[key] = transformer.transform(np.array(list(timeseries1[diz].values())).reshape(1,-1))\n",
    "\n",
    "       \n",
    "#%%  TESTS\n",
    "       \n",
    "#import time\n",
    "#start = time.time()\n",
    "#for i in range (1000):\n",
    "#    text_preprocessing(tweetDict['full_text'])\n",
    "#end = time.time() \n",
    "#print(end-start)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_flatted = pd.DataFrame.from_dict({(i,j): timeseries[i][j] \n",
    "                           for i in timeseries.keys() \n",
    "                           for j in timeseries[i].keys()},\n",
    "                       orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetimeseries_flatted=timeseries_flatted.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetimeseries_flatted = timetimeseries_flatted.rename(columns = {'index':'filename'})\n",
    "timetimeseries_flatted = timetimeseries_flatted.rename(columns = {0:'value'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetimeseries_flatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(timetimeseries_flatted['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetimeseries_flatted.filename.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_list = list(timetimeseries_flatted.filename.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'values_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-95bb65709934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalues_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'values_list' is not defined"
     ]
    }
   ],
   "source": [
    "values_list.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
